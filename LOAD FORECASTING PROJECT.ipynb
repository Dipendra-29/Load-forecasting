{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD FORECASTING PROJECT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ6NYCUzaHSu"
   },
   "source": [
    "**Loading** **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3605,
     "status": "ok",
     "timestamp": 1721920135702,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "_dO2_kcXUW1p",
    "outputId": "43dfc10f-9cbe-489b-f110-c0b84d99ce33"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Data = pd.read_csv('continuous dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4W20muaQ2k"
   },
   "source": [
    "**Test-train Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1721920140666,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "m3Ly7jyjYcES"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(Data, test_size=0.2, random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk-9sRTPadCN"
   },
   "source": [
    "**Generating Lagged Features and Rolling Statistics for Time Series Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3367,
     "status": "ok",
     "timestamp": 1721920146592,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "V3ae_MYaXuja",
    "outputId": "d63d1fa7-9e15-4144-8093-d0d28def5edb"
   },
   "outputs": [],
   "source": [
    "windows = [12, 24, 128]\n",
    "for column in train_df.columns:\n",
    "    if column == 'nat_demand':\n",
    "        for window in windows:\n",
    "            train_df[f\"{column}_lag_{window}\"] = train_df[column].shift(window)\n",
    "            train_df[f\"{column}_ma_mean{window}\"] = train_df[column].rolling(window).mean()\n",
    "            train_df[f\"{column}_std_std{window}\"] = train_df[column].rolling(window).std()\n",
    "            train_df[f\"{column}_ewm_std{window}\"] = train_df[column].ewm(window).std()\n",
    "            train_df[f\"{column}_ewm_mean{window}\"] = train_df[column].ewm(window).mean()\n",
    "\n",
    "\n",
    "    if column != 'datetime' and column != 'holiday' and column != 'school' and column != 'Holiday_ID' and column != 'nat_demand':\n",
    "        for window in windows:\n",
    "            train_df[f\"{column}_lag_{window}\"] = train_df[column].shift(window)\n",
    "            train_df[f\"{column}_ma_mean{window}\"] = train_df[column].rolling(window).mean()\n",
    "            train_df[f\"{column}_std_std{window}\"] = train_df[column].rolling(window).std()\n",
    "            train_df[f\"{column}_ewm_std{window}\"] = train_df[column].ewm(window).std()\n",
    "            train_df[f\"{column}_ewm_mean{window}\"] = train_df[column].ewm(window).mean()\n",
    "            train_df[f\"{column}_min_max{window}\"] = (train_df[column] -train_df[column].rolling(window).min()) / (train_df[column].rolling(window).max() - train_df[column].rolling(window).min())\n",
    "            train_df[f\"{column}_median{window}\"] = train_df[column].rolling(window).median()\n",
    "            train_df[f\"{column}_skew{window}\"] = train_df[column].rolling(window).skew()\n",
    "            train_df[f\"{column}_kurt{window}\"] = train_df[column].rolling(window).kurt()\n",
    "            train_df[f\"{column}_p50{window}\"] = train_df[column].rolling(window).quantile(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1721920149374,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "zpHusdIOX7qW",
    "outputId": "d6e7da14-f8ad-4c8a-caa1-37ea16b6a8d3"
   },
   "outputs": [],
   "source": [
    "windows = [12, 24, 128]\n",
    "for column in test_df.columns:\n",
    "    if column == 'nat_demand':\n",
    "        for window in windows:\n",
    "            test_df[f\"{column}_lag_{window}\"] = test_df[column].shift(window)\n",
    "            test_df[f\"{column}_ma_mean{window}\"] = test_df[column].rolling(window).mean()\n",
    "            test_df[f\"{column}_std_std{window}\"] = test_df[column].rolling(window).std()\n",
    "            test_df[f\"{column}_ewm_std{window}\"] = test_df[column].ewm(window).std()\n",
    "            test_df[f\"{column}_ewm_mean{window}\"] = test_df[column].ewm(window).mean()\n",
    "\n",
    "\n",
    "    if column != 'datetime' and column != 'holiday' and column != 'school' and column != 'Holiday_ID' and column != 'nat_demand':\n",
    "        for window in windows:\n",
    "            test_df[f\"{column}_lag_{window}\"] = test_df[column].shift(window)\n",
    "            test_df[f\"{column}_ma_mean{window}\"] = test_df[column].rolling(window).mean()\n",
    "            test_df[f\"{column}_std_std{window}\"] = test_df[column].rolling(window).std()\n",
    "            test_df[f\"{column}_ewm_std{window}\"] = test_df[column].ewm(window).std()\n",
    "            test_df[f\"{column}_ewm_mean{window}\"] = test_df[column].ewm(window).mean()\n",
    "            test_df[f\"{column}_min_max{window}\"] = (test_df[column] -test_df[column].rolling(window).min()) / (test_df[column].rolling(window).max() - test_df[column].rolling(window).min())\n",
    "            test_df[f\"{column}_median{window}\"] = test_df[column].rolling(window).median()\n",
    "            test_df[f\"{column}_skew{window}\"] = test_df[column].rolling(window).skew()\n",
    "            test_df[f\"{column}_kurt{window}\"] = test_df[column].rolling(window).kurt()\n",
    "            test_df[f\"{column}_p50{window}\"] = test_df[column].rolling(window).quantile(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ4_r4uNayGv"
   },
   "source": [
    "**Removing Rows with Missing Values from Training and Testing DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1721920154915,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "jBOiBoIUYptd"
   },
   "outputs": [],
   "source": [
    "train_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721920157435,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "Fzd1abXDYxQG"
   },
   "outputs": [],
   "source": [
    "test_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721920159788,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "DgGR-WkkYzZa"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_df.drop(columns = ['nat_demand','datetime']), test_df.drop(columns = ['nat_demand','datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721920162132,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "1T-c5uaUY10f"
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_df['nat_demand'], test_df['nat_demand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization {Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1721920183380,
     "user": {
      "displayName": "Dipendra Yadav",
      "userId": "18060708490325037341"
     },
     "user_tz": -330
    },
    "id": "ApOWeb61Y62X"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### formation of d-matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train, feature_names=X_train.columns.tolist())\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test, feature_names=X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining objective function (xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0,\n",
    "        'eta': trial.suggest_float('eta', 0.001, 0.5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'lambda': trial.suggest_float('lambda', 0.0, 2.0),\n",
    "        'alpha': trial.suggest_float('alpha', 0.0, 2.0),\n",
    "        'nthread': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    num_round = 100\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    predictions = model.predict(dtest)\n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:09:51,240] A new study created in memory with name: no-name-c344ee46-6995-45ee-a19e-1901bc8bfb2d\n",
      "[I 2024-07-26 12:10:19,277] Trial 0 finished with value: 123.29504309645017 and parameters: {'eta': 0.3756137657135743, 'max_depth': 12, 'min_child_weight': 19.338400208286004, 'subsample': 0.6932082849673241, 'colsample_bytree': 0.8312541473509123, 'gamma': 2.615262557101587, 'lambda': 0.2894711663289764, 'alpha': 0.7621549352149095}. Best is trial 0 with value: 123.29504309645017.\n",
      "[I 2024-07-26 12:11:05,435] Trial 1 finished with value: 109.8067608782087 and parameters: {'eta': 0.026289933367636575, 'max_depth': 12, 'min_child_weight': 8.321509807728463, 'subsample': 0.7270597153709595, 'colsample_bytree': 0.6922478160608861, 'gamma': 2.1894020796062375, 'lambda': 1.1317558871069473, 'alpha': 1.5279580917047217}. Best is trial 1 with value: 109.8067608782087.\n",
      "[I 2024-07-26 12:11:44,300] Trial 2 finished with value: 116.70342292268056 and parameters: {'eta': 0.34692587210069886, 'max_depth': 15, 'min_child_weight': 14.367671353763154, 'subsample': 0.9833203670100821, 'colsample_bytree': 0.7805477487697392, 'gamma': 3.0040020894823227, 'lambda': 0.5416105569331593, 'alpha': 0.3375258723401937}. Best is trial 1 with value: 109.8067608782087.\n",
      "[I 2024-07-26 12:11:48,192] Trial 3 finished with value: 114.94256440033936 and parameters: {'eta': 0.3681638046959383, 'max_depth': 7, 'min_child_weight': 12.845831398936514, 'subsample': 0.7965689611177018, 'colsample_bytree': 0.7901023721438946, 'gamma': 4.044030434314087, 'lambda': 0.9312013051540746, 'alpha': 0.5714510008025868}. Best is trial 1 with value: 109.8067608782087.\n",
      "[I 2024-07-26 12:11:49,399] Trial 4 finished with value: 120.0714692533921 and parameters: {'eta': 0.022485184461880227, 'max_depth': 4, 'min_child_weight': 7.532875298856412, 'subsample': 0.7378448462998912, 'colsample_bytree': 0.7051459434185714, 'gamma': 2.841207598361138, 'lambda': 1.3791367096057858, 'alpha': 0.27782031724029355}. Best is trial 1 with value: 109.8067608782087.\n",
      "[I 2024-07-26 12:12:28,554] Trial 5 finished with value: 111.92148192534586 and parameters: {'eta': 0.20930549205846627, 'max_depth': 13, 'min_child_weight': 8.023532168980278, 'subsample': 0.9998143612368348, 'colsample_bytree': 0.5940020316219684, 'gamma': 2.987838476225039, 'lambda': 0.7590234675103753, 'alpha': 0.12158736248470947}. Best is trial 1 with value: 109.8067608782087.\n",
      "[I 2024-07-26 12:13:28,602] Trial 6 finished with value: 109.31392671526118 and parameters: {'eta': 0.025986680836640873, 'max_depth': 12, 'min_child_weight': 6.184543590932268, 'subsample': 0.6726354703576862, 'colsample_bytree': 0.9627229727528602, 'gamma': 4.206688060775491, 'lambda': 0.6854988842715368, 'alpha': 0.16650191198480924}. Best is trial 6 with value: 109.31392671526118.\n",
      "[I 2024-07-26 12:14:14,803] Trial 7 finished with value: 115.78760602164 and parameters: {'eta': 0.22586162714162503, 'max_depth': 20, 'min_child_weight': 18.611754349996353, 'subsample': 0.5721724129466035, 'colsample_bytree': 0.9297595716775228, 'gamma': 4.137903107861699, 'lambda': 1.4354244700568726, 'alpha': 1.1716440209686265}. Best is trial 6 with value: 109.31392671526118.\n",
      "[I 2024-07-26 12:15:41,809] Trial 8 finished with value: 108.51359130418835 and parameters: {'eta': 0.05308986243816068, 'max_depth': 18, 'min_child_weight': 9.844166598031835, 'subsample': 0.8015901581323165, 'colsample_bytree': 0.6402039174898675, 'gamma': 3.503961771812809, 'lambda': 0.8315398017648226, 'alpha': 1.941529698618804}. Best is trial 8 with value: 108.51359130418835.\n",
      "[I 2024-07-26 12:17:47,551] Trial 9 finished with value: 107.96740431836238 and parameters: {'eta': 0.042385612010180464, 'max_depth': 20, 'min_child_weight': 9.067183102166886, 'subsample': 0.74024831902097, 'colsample_bytree': 0.9887975245714852, 'gamma': 3.4140152265325563, 'lambda': 0.41403538927045624, 'alpha': 1.025166769192781}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:17:53,486] Trial 10 finished with value: 120.72451949875659 and parameters: {'eta': 0.4868281565775011, 'max_depth': 8, 'min_child_weight': 1.197388698213957, 'subsample': 0.8963208962016823, 'colsample_bytree': 0.5358046660881532, 'gamma': 0.633357727473497, 'lambda': 1.8978478327788237, 'alpha': 1.0780119341582084}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:19:47,131] Trial 11 finished with value: 112.79175015433209 and parameters: {'eta': 0.12486391394426502, 'max_depth': 20, 'min_child_weight': 3.5796850157876055, 'subsample': 0.8342365498911981, 'colsample_bytree': 0.6044639263835181, 'gamma': 4.786306095419198, 'lambda': 0.09144652860166269, 'alpha': 1.8177799200287277}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:20:42,038] Trial 12 finished with value: 110.86704278036548 and parameters: {'eta': 0.13060046185194646, 'max_depth': 17, 'min_child_weight': 12.286290274506047, 'subsample': 0.6381055834040825, 'colsample_bytree': 0.8930544028479124, 'gamma': 1.6358391157517826, 'lambda': 0.3469266750624882, 'alpha': 1.9629337852449416}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:21:51,004] Trial 13 finished with value: 108.70018597333708 and parameters: {'eta': 0.12718134759080063, 'max_depth': 18, 'min_child_weight': 15.288541546116516, 'subsample': 0.8753775900306886, 'colsample_bytree': 0.9959019562854028, 'gamma': 3.566851771340275, 'lambda': 1.0793377389683128, 'alpha': 1.4717789452013432}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:22:39,174] Trial 14 finished with value: 110.35342150136375 and parameters: {'eta': 0.08867745966617033, 'max_depth': 16, 'min_child_weight': 9.779016126053858, 'subsample': 0.5392046402541669, 'colsample_bytree': 0.6609606833667672, 'gamma': 1.7745513119251277, 'lambda': 0.4727328159190648, 'alpha': 1.4391194383513124}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:24:07,629] Trial 15 finished with value: 114.75302386298921 and parameters: {'eta': 0.18177444700398457, 'max_depth': 19, 'min_child_weight': 5.552635393743604, 'subsample': 0.7847873423749929, 'colsample_bytree': 0.8615270587762924, 'gamma': 3.472342277290108, 'lambda': 0.09390205649316519, 'alpha': 0.6839991285178922}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:24:55,452] Trial 16 finished with value: 108.96776649958593 and parameters: {'eta': 0.07260963261197242, 'max_depth': 15, 'min_child_weight': 10.608965931036522, 'subsample': 0.6327352641742323, 'colsample_bytree': 0.6098377941436812, 'gamma': 4.778653790237941, 'lambda': 0.7771675081761191, 'alpha': 1.651001167120564}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:26:00,604] Trial 17 finished with value: 118.29066938543453 and parameters: {'eta': 0.28268585528214474, 'max_depth': 18, 'min_child_weight': 4.2759656763933425, 'subsample': 0.9226937504666266, 'colsample_bytree': 0.5361514732395957, 'gamma': 0.2476476987962668, 'lambda': 1.337964408634049, 'alpha': 1.2044650796313532}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:26:06,013] Trial 18 finished with value: 110.4958704374904 and parameters: {'eta': 0.28763635899244067, 'max_depth': 8, 'min_child_weight': 16.864224428598952, 'subsample': 0.8061200792423366, 'colsample_bytree': 0.7412964482152827, 'gamma': 3.4657130354201167, 'lambda': 1.739465306262548, 'alpha': 0.9388984503253988}. Best is trial 9 with value: 107.96740431836238.\n",
      "[I 2024-07-26 12:27:26,637] Trial 19 finished with value: 166.006927970267 and parameters: {'eta': 0.002664867825888595, 'max_depth': 15, 'min_child_weight': 10.856552375669038, 'subsample': 0.8559842033006205, 'colsample_bytree': 0.6541622637067509, 'gamma': 1.2018288485785644, 'lambda': 0.9119918793827914, 'alpha': 1.274539532143653}. Best is trial 9 with value: 107.96740431836238.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'eta': 0.042385612010180464, 'max_depth': 20, 'min_child_weight': 9.067183102166886, 'subsample': 0.74024831902097, 'colsample_bytree': 0.9887975245714852, 'gamma': 3.4140152265325563, 'lambda': 0.41403538927045624, 'alpha': 1.025166769192781, 'tree_method': 'gpu_hist', 'gpu_id': 0}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params['tree_method'] = 'gpu_hist'\n",
    "best_params['gpu_id'] = 0\n",
    "print(\"Best params:\", best_params)\n",
    "xgb_best_params = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.train(best_params, dtrain, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data: 107.80262871991268\n",
      "MAE on test data: 84.43909390960286\n",
      "Mean Absolute Percentage Error: 0.07386719302404203\n",
      "R-squared on test data: 0.6856678657443636\n",
      "Explained Variance on test data: 0.68569659900662\n",
      "Max Error on test data: 1007.59040390625\n",
      "Mean Poisson Deviance on test data: 9.84393125184069\n",
      "Mean Gamma Deviance on test data: 0.008559882795658027\n",
      "Mean Tweedie Deviance on test data: 11621.40675892334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, \\\n",
    "    max_error, mean_poisson_deviance, mean_gamma_deviance, mean_tweedie_deviance, mean_absolute_percentage_error\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, predictions, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, predictions)\n",
    "r2_test = r2_score(y_test, predictions)\n",
    "explained_variance = explained_variance_score(y_test, predictions)\n",
    "max_err = max_error(y_test, predictions)\n",
    "poisson_deviance = mean_poisson_deviance(y_test, predictions)\n",
    "gamma_deviance = mean_gamma_deviance(y_test, predictions)\n",
    "tweedie_deviance = mean_tweedie_deviance(y_test, predictions)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "print(f\"efficiency: {[1 - mae_test]*100})\n",
    "print(f\"RMSE on test data: {rmse_test}\")\n",
    "print(f\"MAE on test data: {mae_test}\")\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n",
    "print(f\"R-squared on test data: {r2_test}\")\n",
    "print(f\"Explained Variance on test data: {explained_variance}\")\n",
    "print(f\"Max Error on test data: {max_err}\")\n",
    "print(f\"Mean Poisson Deviance on test data: {poisson_deviance}\")\n",
    "print(f\"Mean Gamma Deviance on test data: {gamma_deviance}\")\n",
    "print(f\"Mean Tweedie Deviance on test data: {tweedie_deviance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formation of lgb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(X_train_scaled, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_scaled, label=y_test, reference=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Objective function (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 64),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'num_boost_round': 1000,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'device': 'cpu',  # Use GPU for training\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:32:56,856] A new study created in memory with name: no-name-c6f663e2-1fbc-4aab-b3b8-69c53757cb7a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:33:15,498] Trial 0 finished with value: 103.67270599425187 and parameters: {'num_leaves': 48, 'learning_rate': 0.04692662204134025, 'feature_fraction': 0.7376093034130742, 'bagging_fraction': 0.5946904375729118, 'bagging_freq': 2}. Best is trial 0 with value: 103.67270599425187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's rmse: 103.673\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:33:26,248] Trial 1 finished with value: 102.99844929196445 and parameters: {'num_leaves': 28, 'learning_rate': 0.07931549695224117, 'feature_fraction': 0.7567388981800469, 'bagging_fraction': 0.776058057221476, 'bagging_freq': 2}. Best is trial 1 with value: 102.99844929196445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[721]\tvalid_0's rmse: 102.998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:33:37,937] Trial 2 finished with value: 105.46444382702241 and parameters: {'num_leaves': 27, 'learning_rate': 0.013354767252559279, 'feature_fraction': 0.5265341919832445, 'bagging_fraction': 0.6162445486964399, 'bagging_freq': 1}. Best is trial 1 with value: 102.99844929196445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 105.464\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:33:44,417] Trial 3 finished with value: 106.76353266327597 and parameters: {'num_leaves': 44, 'learning_rate': 0.09725443645057019, 'feature_fraction': 0.5953265128975445, 'bagging_fraction': 0.5124731643661857, 'bagging_freq': 9}. Best is trial 1 with value: 102.99844929196445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's rmse: 106.764\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:34:02,275] Trial 4 finished with value: 103.79591780249655 and parameters: {'num_leaves': 52, 'learning_rate': 0.06772693001978898, 'feature_fraction': 0.9394273285493433, 'bagging_fraction': 0.6519182198875291, 'bagging_freq': 8}. Best is trial 1 with value: 102.99844929196445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[584]\tvalid_0's rmse: 103.796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's rmse: 101.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:34:35,875] Trial 5 finished with value: 101.84810745241991 and parameters: {'num_leaves': 57, 'learning_rate': 0.034151254281399444, 'feature_fraction': 0.9219470837278243, 'bagging_fraction': 0.989866891630629, 'bagging_freq': 9}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:35:08,883] Trial 6 finished with value: 103.00058131562726 and parameters: {'num_leaves': 40, 'learning_rate': 0.01693409132986115, 'feature_fraction': 0.8521439449560184, 'bagging_fraction': 0.765915835603619, 'bagging_freq': 10}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's rmse: 103.001\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:35:19,769] Trial 7 finished with value: 102.91589731945521 and parameters: {'num_leaves': 46, 'learning_rate': 0.06953234640795944, 'feature_fraction': 0.7874326525828075, 'bagging_fraction': 0.8031135274036425, 'bagging_freq': 4}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[413]\tvalid_0's rmse: 102.916\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:35:26,776] Trial 8 finished with value: 104.53155616896595 and parameters: {'num_leaves': 55, 'learning_rate': 0.08345860324935771, 'feature_fraction': 0.8225238228478309, 'bagging_fraction': 0.6107859827057074, 'bagging_freq': 3}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's rmse: 104.532\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:35:44,716] Trial 9 finished with value: 105.93339877216056 and parameters: {'num_leaves': 52, 'learning_rate': 0.006797346728814194, 'feature_fraction': 0.5506765250737877, 'bagging_fraction': 0.5657243091511359, 'bagging_freq': 1}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 105.933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:35:52,566] Trial 10 finished with value: 106.78425902177307 and parameters: {'num_leaves': 7, 'learning_rate': 0.03617580195770336, 'feature_fraction': 0.9646772196825344, 'bagging_fraction': 0.968484534552954, 'bagging_freq': 7}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 106.784\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:36:15,233] Trial 11 finished with value: 102.26200396096323 and parameters: {'num_leaves': 64, 'learning_rate': 0.058971160483864735, 'feature_fraction': 0.6813260686732793, 'bagging_fraction': 0.9971551300355685, 'bagging_freq': 5}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[650]\tvalid_0's rmse: 102.262\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:36:40,976] Trial 12 finished with value: 102.09804474269957 and parameters: {'num_leaves': 62, 'learning_rate': 0.03316480758461893, 'feature_fraction': 0.6431043481364207, 'bagging_fraction': 0.9946158059459578, 'bagging_freq': 6}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 102.098\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:37:04,069] Trial 13 finished with value: 101.93488603244123 and parameters: {'num_leaves': 61, 'learning_rate': 0.029583087091448138, 'feature_fraction': 0.6452159049051135, 'bagging_fraction': 0.8888907228018502, 'bagging_freq': 6}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's rmse: 101.935\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:37:30,270] Trial 14 finished with value: 102.55605717056523 and parameters: {'num_leaves': 35, 'learning_rate': 0.027538162570958227, 'feature_fraction': 0.9004046172597813, 'bagging_fraction': 0.8790648638309966, 'bagging_freq': 7}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[995]\tvalid_0's rmse: 102.556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064119 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:37:50,561] Trial 15 finished with value: 102.82733016858171 and parameters: {'num_leaves': 58, 'learning_rate': 0.05032799623984032, 'feature_fraction': 0.6971158299255474, 'bagging_fraction': 0.897869652539974, 'bagging_freq': 10}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[695]\tvalid_0's rmse: 102.827\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.047821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:37:55,634] Trial 16 finished with value: 112.32445628640731 and parameters: {'num_leaves': 4, 'learning_rate': 0.024250706633501106, 'feature_fraction': 0.6087986584339761, 'bagging_fraction': 0.8996671317549724, 'bagging_freq': 8}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 112.324\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:38:08,614] Trial 17 finished with value: 104.44342660042416 and parameters: {'num_leaves': 13, 'learning_rate': 0.037926464915145064, 'feature_fraction': 0.9990101121158986, 'bagging_fraction': 0.8411775652552076, 'bagging_freq': 5}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 104.443\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:38:52,843] Trial 18 finished with value: 109.00889970227763 and parameters: {'num_leaves': 59, 'learning_rate': 0.0031669157188069785, 'feature_fraction': 0.8909600546829083, 'bagging_fraction': 0.7059731937879508, 'bagging_freq': 6}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 109.009\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-26 12:39:08,396] Trial 19 finished with value: 103.85596247681093 and parameters: {'num_leaves': 20, 'learning_rate': 0.042564026434113, 'feature_fraction': 0.7078976962387798, 'bagging_fraction': 0.9369495173495703, 'bagging_freq': 9}. Best is trial 5 with value: 101.84810745241991.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[994]\tvalid_0's rmse: 103.856\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'num_leaves': 57, 'learning_rate': 0.034151254281399444, 'feature_fraction': 0.9219470837278243, 'bagging_fraction': 0.989866891630629, 'bagging_freq': 9}\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "lgb_best_params = best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98712\n",
      "[LightGBM] [Info] Number of data points in the train set: 38310, number of used features: 390\n",
      "[LightGBM] [Info] Start training from score 1183.328632\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.train(best_params, train_data, valid_sets=[test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data: 108.29433654033951\n",
      "MAE on test data: 85.5117087475924\n",
      "Mean Absolute Percentage Error: 0.07493469679355141\n",
      "R-squared on test data: 0.6827938717069257\n",
      "Explained Variance on test data: 0.6827957279014445\n",
      "Max Error on test data: 994.2780689639762\n",
      "Mean Poisson Deviance on test data: 9.957943504702682\n",
      "Mean Gamma Deviance on test data: 0.008680603516445335\n",
      "Mean Tweedie Deviance on test data: 11727.663326712314\n"
     ]
    }
   ],
   "source": [
    "predictions = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, predictions, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, predictions)\n",
    "r2_test = r2_score(y_test, predictions)\n",
    "explained_variance = explained_variance_score(y_test, predictions)\n",
    "max_err = max_error(y_test, predictions)\n",
    "poisson_deviance = mean_poisson_deviance(y_test, predictions)\n",
    "gamma_deviance = mean_gamma_deviance(y_test, predictions)\n",
    "tweedie_deviance = mean_tweedie_deviance(y_test, predictions)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "print(f\"RMSE on test data: {rmse_test}\")\n",
    "print(f\"MAE on test data: {mae_test}\")\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n",
    "print(f\"R-squared on test data: {r2_test}\")\n",
    "print(f\"Explained Variance on test data: {explained_variance}\")\n",
    "print(f\"Max Error on test data: {max_err}\")\n",
    "print(f\"Mean Poisson Deviance on test data: {poisson_deviance}\")\n",
    "print(f\"Mean Gamma Deviance on test data: {gamma_deviance}\")\n",
    "print(f\"Mean Tweedie Deviance on test data: {tweedie_deviance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalution scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data: 111.14056568023987\n",
      "MAE on test data: 89.03805885894238\n",
      "Mean Absolute Percentage Error: 0.07788497269705887\n",
      "R-squared on test data: 0.6659009159230549\n",
      "Explained Variance on test data: 0.6659027858491346\n",
      "Max Error on test data: 850.023528072639\n",
      "Mean Poisson Deviance on test data: 10.559710139055353\n",
      "Mean Gamma Deviance on test data: 0.009266376597674871\n",
      "Mean Tweedie Deviance on test data: 12352.225339723713\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "explained_variance = explained_variance_score(y_test, y_pred)\n",
    "max_err = max_error(y_test, y_pred)\n",
    "poisson_deviance = mean_poisson_deviance(y_test, y_pred)\n",
    "gamma_deviance = mean_gamma_deviance(y_test, y_pred)\n",
    "tweedie_deviance = mean_tweedie_deviance(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"RMSE on test data: {rmse_test}\")\n",
    "print(f\"MAE on test data: {mae_test}\")\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n",
    "print(f\"R-squared on test data: {r2_test}\")\n",
    "print(f\"Explained Variance on test data: {explained_variance}\")\n",
    "print(f\"Max Error on test data: {max_err}\")\n",
    "print(f\"Mean Poisson Deviance on test data: {poisson_deviance}\")\n",
    "print(f\"Mean Gamma Deviance on test data: {gamma_deviance}\")\n",
    "print(f\"Mean Tweedie Deviance on test data: {tweedie_deviance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LassoCV()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LassoCV()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "model = LassoCV()\n",
    "model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data: 111.69384819783046\n",
      "MAE on test data: 89.6420296316115\n",
      "Mean Absolute Percentage Error: 0.07846212135494196\n",
      "R-squared on test data: 0.6625661965713914\n",
      "Explained Variance on test data: 0.6625724021916811\n",
      "Max Error on test data: 895.4612779533461\n",
      "Mean Poisson Deviance on test data: 10.650020414830395\n",
      "Mean Gamma Deviance on test data: 0.009330159589739767\n",
      "Mean Tweedie Deviance on test data: 12475.515725239997\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "explained_variance = explained_variance_score(y_test, y_pred)\n",
    "max_err = max_error(y_test, y_pred)\n",
    "poisson_deviance = mean_poisson_deviance(y_test, y_pred)\n",
    "gamma_deviance = mean_gamma_deviance(y_test, y_pred)\n",
    "tweedie_deviance = mean_tweedie_deviance(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"RMSE on test data: {rmse_test}\")\n",
    "print(f\"MAE on test data: {mae_test}\")\n",
    "print(\"Mean Absolute Percentage Error:\", mape)\n",
    "print(f\"R-squared on test data: {r2_test}\")\n",
    "print(f\"Explained Variance on test data: {explained_variance}\")\n",
    "print(f\"Max Error on test data: {max_err}\")\n",
    "print(f\"Mean Poisson Deviance on test data: {poisson_deviance}\")\n",
    "print(f\"Mean Gamma Deviance on test data: {gamma_deviance}\")\n",
    "print(f\"Mean Tweedie Deviance on test data: {tweedie_deviance}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwuzxaqcwVqrKDcC0EqDMd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
